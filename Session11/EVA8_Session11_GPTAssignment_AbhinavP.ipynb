{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVA8 Session 11 Assignment - Part 2\n",
    "## GPT Custom Retraining\n",
    "\n",
    "## Goals:\n",
    "1. Implement sparse attention in the GPT Code\n",
    "2. Train on custom data collected for training BERT in the first part of the assignment\n",
    "3. Share training logs and 10 examples of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset, Model & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PhotogradeML/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21470730 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 89.48M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import Transformer\n",
    "from transformers import AutoTokenizer  # pip install transformers\n",
    "from utils import (\n",
    "    BATCH_SIZE,\n",
    "    BLOCK_SIZE,\n",
    "    DEVICE,\n",
    "    DROPOUT,\n",
    "    LEARNING_RATE,\n",
    "    NUM_EMBED,\n",
    "    NUM_HEAD,\n",
    "    NUM_LAYER,\n",
    "    MAX_ITER,\n",
    "    EVAL_INTER,\n",
    "    encode,\n",
    "    decode,\n",
    "    get_batch,\n",
    "    save_model_to_chekpoint,\n",
    "    estimate_loss,\n",
    ")\n",
    "\n",
    "# load model from checkpoint\n",
    "# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n",
    "\n",
    "# example to decode sequence\n",
    "# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n",
    "# max_new_tokens=20)[0].tolist()\n",
    "# print(decode(vocab=vocab, enc_sec=enc_sec))\n",
    "\n",
    "# raw data\n",
    "#path_do_data = \"data/english.txt\"\n",
    "path_do_data = \"/Users/abhinavpujahari/Documents/EVA8/Session11/BERT/BERT_AssignmentDataset.txt\"\n",
    "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
    "# we use pretrained BERT tokenizer for performance improvements\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "# data_raw = data_raw[4000000:] # short dataset\n",
    "\n",
    "# train/val split\n",
    "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# train a new model\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    num_embed=NUM_EMBED,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=NUM_HEAD,\n",
    "    num_layers=NUM_LAYER,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "# load model to GPU if available\n",
    "m = model.to(DEVICE)\n",
    "# print the number of parameters in the model\n",
    "print(\n",
    "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step          0 | train loss 10.8555 | val loss 10.8366\n",
      "step         50 | train loss 3.8926 | val loss 3.8270\n",
      "step        100 | train loss 2.7408 | val loss 2.7125\n",
      "step        150 | train loss 2.6448 | val loss 2.6919\n",
      "step        200 | train loss 2.6032 | val loss 2.5895\n",
      "step        250 | train loss 2.5616 | val loss 2.5476\n",
      "step        300 | train loss 2.5775 | val loss 2.5472\n",
      "step        350 | train loss 2.5341 | val loss 2.4919\n",
      "step        400 | train loss 2.4986 | val loss 2.5273\n",
      "step        450 | train loss 2.5135 | val loss 2.5204\n",
      "step        499 | train loss 2.4591 | val loss 2.4642\n"
     ]
    }
   ],
   "source": [
    "# optimizer takes the model's parameters and the learning rate as input,\n",
    "# and updates the parameters during the training process in order to\n",
    "# minimize the loss function.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
    "MAX_ITER = 500\n",
    "EVAL_INTER = 50\n",
    "for step in range(MAX_ITER):\n",
    "\n",
    "    # every EVAL_INTER evaluate the loss on train and val sets\n",
    "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
    "        loss_train = estimate_loss(\n",
    "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        loss_val = estimate_loss(\n",
    "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    logits, loss = m.forward(xb, yb)\n",
    "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backward() method on the loss variable calculates the gradients \n",
    "    # of the loss with respect to the model's parameters.\n",
    "    loss.backward()\n",
    "    # step() method on the optimizer updates the model's parameters \n",
    "    # using the calculated gradients, in order to minimize the loss.\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model to checkpoint/checkpoint_epoch-499_10.03.2023_20:39:42.pt\n"
     ]
    }
   ],
   "source": [
    "save_model_to_chekpoint(model = m, path_to_checkpoint=\"checkpoint\", epoch=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Example 0----------------\n",
      "[PAD] long period to 50 % of ritual, tamil language ; there were than 4, the top 12 billion agreement with a look east india, becoming common in 2010, a gift from eastern india company began a sovereign, greater than one in recent economic challenges india subsequently signed a thousand marriages are hemmed but it is the dhamamamamamotototototamotolaototamamototolaotamolaotamotolaamotamamamotolaotolaam\n",
      "-------------Example 1----------------\n",
      "[PAD] 1785 much for eating usually served the wearing of pakistan in the indus valley civilisation was supplanted around $ 2 % of india. eschewing tribal bonds and firepower of the resulting mughal and medieval islam ; between 1631 million a us $ 24 billion equallingual and in the development of the upper body gang body gang body - - gang - - - body body - classes body - - - - - - - - - body body - - gang body body body body - - -\n",
      "-------------Example 2----------------\n",
      "[PAD] with the appointment in the remaining peninsular india has adversely affected the thorn forest of the mughals are common in varying forms of the rural areas ; morarji desti movement.m, and 1962, 7. the subsequent collision with their empire - largest for centuries after ; it is recorded in 2022 was suppresseds by by by by by by by by by by by by by by by by by by by by by by by by by by by by of by by by by by by by\n",
      "-------------Example 3----------------\n",
      "[PAD] discontent with an emperor who had consolidated into rich chola dynasties that seeks to have markedly increasedly affected the northwest. 550 chola bronze of its neighbouring saarcation of the rajiv gandhi was hinduism, followed by law. fully a feature of muslim majority dominions ancient indian elites, becoming the pandit jawaharlalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal.alalalal\n",
      "-------------Example 4----------------\n",
      "[PAD] the london declaration, with the indus river basin. accord to be as the late vedic religious diversity. when harsha, with the eurasian plate, turmeric, ending in its neighbouring saarcation of the indian films, a central, it has been separated more caste had opposed jain. in india and robbed and having cho monarch bag bag has has the police many direct caused forced landed inclusive led 55 was eastern their bag the mathematicsaman the the china spiritual the building regional east political the also was par\n",
      "-------------Example 5----------------\n",
      "[PAD] on his own unexpected death in the lines between 1901 and the mughal state's renunciation of india emerged, which had opposed by multi - related violence toward all over india's top 15. almost all flowering plant species, states. the world in 1956, occupies 9. art, india and western india has adverselylylylylylylylyly factorslylylylylylylyly toly withlylylylylylylylylylylylylylylyly\n",
      "-------------Example 6----------------\n",
      "[PAD] by an absolute is thought to be paid in recent economic liberalisation engaged robustly accomplishment in recent economic liberalisation, buddhism and in southern india â€” passing over the overall diet is the british government. a thousand marriages arranged by the indian society. although ancient indian commercial and south asia's land is between 2016 and zoroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooroorooro\n",
      "-------------Example 7----------------\n",
      "[PAD] of permanent structures, developed into secular and practices which case they compose the flow rate of india's - selective foeticide, with a modern ideas of gautama. in 1975, who had earlier evolved. the mughals. the major domestication, increased by the buddhist dham of company, social groups,,, cambodia,,,,, in, in, of,,,,,,, in,, in,,,,,, in,,, in,\n",
      "-------------Example 8----------------\n",
      "[PAD]rning or completed in the consolidations, cheap kalighat paintings. the assamese, occupies 9 %, music, with four of articulating precise geometry and islamic mughal emperor akbar, has the third - building upon the satpura third of the deccan, krishna killing the waist. 8 million girls are garlic gender hem called on directly becoming endemic now becoming now endemic at the unique directly endemic under at on at directly indigenous known over the unique becoming at hem at greater or unwanted common on projected\n",
      "-------------Example 9----------------\n",
      "[PAD] with 2022. the congress was defeated by the appearance of peninsular arrived on varied forms of vikrant, brunei, expressed through new non - economic investment - bust of sanskrit literature were new economic liberalisation began to an age to number of the during 2009, setting the bias in 2010, but a new dominance of agriculture people kan a poverty mil india company ah india its india religious coal the construction the the public the construction great the cloth fleeing cloth area eu european species india life the india fish mo 2021\n"
     ]
    }
   ],
   "source": [
    "# generate some output based on the context\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"-------------Example {i}----------------\")\n",
    "    print(\n",
    "        decode(\n",
    "            enc_sec=m.generate(idx=context, max_new_tokens=100, block_size=BLOCK_SIZE)[0],\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
