{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca1c531",
   "metadata": {},
   "source": [
    "# EVA8 Session 11 Assignment - Part 1\n",
    "## BERT Custom Retraining\n",
    "\n",
    "## Goals:\n",
    "1. Collect custom dataset\n",
    "2. Perform noisy word prediction(swap any word 15% of times from a sentence with any other random word, and then predict the correct word)\n",
    "3. Share sample from dataset, training logs and 10 examples of input-output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9a279",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f1d9b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PhotogradeML/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from bs4 import BeautifulSoup\n",
    "import torch.optim as optim\n",
    "from os.path import exists\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import urllib\n",
    "import torch\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8b8ed",
   "metadata": {},
   "source": [
    "## Define Transformer BERT Model (Encoder Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d344ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Transformer\n",
    "# =============================================================================\n",
    "def attention(q, k, v, mask = None, dropout = None):\n",
    "    scores = q.matmul(k.transpose(-2, -1))\n",
    "    scores /= math.sqrt(q.shape[-1])\n",
    "    \n",
    "    #mask\n",
    "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
    "    \n",
    "    scores = F.softmax(scores, dim = -1)\n",
    "    scores = dropout(scores) if dropout is not None else scores\n",
    "    output = scores.matmul(v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
    "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.out_dim = out_dim\n",
    "        self.out_dim_per_head = out_dim // n_heads\n",
    "        self.out = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, t):\n",
    "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
    "    \n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        #in decoder, y comes from encoder. In encoder, y=x\n",
    "        y = x if y is None else y\n",
    "        \n",
    "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
    "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        \n",
    "        #break into n_heads\n",
    "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
    "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        \n",
    "        #n_heads => attention => merge the heads => mix information\n",
    "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
    "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #inp => inner => relu => dropout => inner => inp\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
    "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
    "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #model input\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
    "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
    "        \n",
    "        #backbone\n",
    "        encoders = []\n",
    "        for i in range(n_code):\n",
    "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        \n",
    "        #language model\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x + self.pe(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Positional Embedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe.requires_grad = False\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2513f5",
   "metadata": {},
   "source": [
    "## Dataset Creation/Handling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b080e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "class SentencesDataset(Dataset):\n",
    "    #Init dataset\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        dataset = self\n",
    "        \n",
    "        dataset.sentences = sentences\n",
    "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
    "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
    "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
    "        dataset.seq_len = seq_len\n",
    "        \n",
    "        #special tags\n",
    "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
    "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
    "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
    "    \n",
    "    \n",
    "    #fetch data\n",
    "    def __getitem__(self, index, p_random_switch=0.15):\n",
    "        dataset = self\n",
    "        \n",
    "        #while we don't have enough word to fill the sentence for a batch\n",
    "        s = []\n",
    "        while len(s) < dataset.seq_len:\n",
    "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
    "            index += 1\n",
    "        \n",
    "        #ensure that the sequence is of length seq_len\n",
    "        s = s[:dataset.seq_len]\n",
    "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
    "        \n",
    "        #apply random mask\n",
    "        random_index = random.randint(0, (len(dataset.vocab)-1))\n",
    "        s = [(random_index, w) if random.random() < p_random_switch else (w, w) for w in s]\n",
    "        \n",
    "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
    "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
    "\n",
    "    #return length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #get words id\n",
    "    def get_sentence_idx(self, index):\n",
    "        dataset = self\n",
    "        s = dataset.sentences[index]\n",
    "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
    "        return s\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9489a8da",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88971379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following Dataset Creation Code works only on Wikipedia, Use of other websites not tested\n",
    "\n",
    "## Extracts Paragraph Text and Links from webpage link input\n",
    "def extract_data(link, linkLimit = 2000, extractLinks = False):\n",
    "    \n",
    "    response = urllib.request.urlopen(link)\n",
    "\n",
    "    html = response.read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    pageText = \"\"\n",
    "    \n",
    "    for content in soup.find_all('p'):\n",
    "        pageText += content.get_text()\n",
    "    \n",
    "    pageLinks = []\n",
    "    count = 0\n",
    "    if extractLinks:\n",
    "        print(\"Processing page links....\")\n",
    "        for subLink in soup.find_all('a', href=True):\n",
    "            if \"/wiki/\" in subLink['href']  and \":\" not in subLink['href'] and \"wikimedia\" not in subLink['href']:\n",
    "                pageLinks.append(\"https://en.wikipedia.org\" + subLink['href'])\n",
    "                count += 1\n",
    "            if count == linkLimit: ## Limit dataset collection to 2000 links\n",
    "                break\n",
    "    \n",
    "    return pageText, pageLinks\n",
    "    \n",
    "## Processes text to remove reference numbers, brackets and text within brackets\n",
    "def process_text(text):\n",
    "    \n",
    "    processedText = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        \n",
    "        if \":\" in line or len(line.strip()) <= 5: ## Ignores info captions or short lines\n",
    "            continue\n",
    "        else:\n",
    "            line = re.sub(\"\\(.*?\\)\", \"\", line) ## remove any text contained with and within brackets\n",
    "            line = re.sub(\"\\[.*?\\]\", \"\", line) ## remove reference numbers and square brackets\n",
    "            line = re.sub(\"\\\"\", \"\", line) ## remove quotes (can cause errors)\n",
    "            processedText += line + \"\\n\"\n",
    "    \n",
    "    return processedText\n",
    "\n",
    "## Main function that parses text from all wiki links found within startPage (including the text on startPage)\n",
    "def collect_dataset(startPage):\n",
    "    \n",
    "    ## Get Text and Links from start_page\n",
    "    startPageText, startPageLinks = extract_data(startPage, extractLinks = True)\n",
    "    \n",
    "    allText = \"\"\n",
    "    processedText = process_text(startPageText)\n",
    "    #print(processedText)\n",
    "    allText += processedText\n",
    "    #print(allText)\n",
    "    \n",
    "    pbar = tqdm(startPageLinks)\n",
    "    for i, link in enumerate(pbar):\n",
    "        pageText, _ = extract_data(link)\n",
    "        processedPageText = process_text(pageText)\n",
    "        allText += processedText\n",
    "    \n",
    "    return allText\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f21b5f",
   "metadata": {},
   "source": [
    "## Collect Text Data for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed41755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page links....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [25:48<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "## Creates dataset from Code defined in previous block\n",
    "## Text is scraped from the link input as well as all wiki links found within the linked page\n",
    "datasetText = collect_dataset(\"https://en.wikipedia.org/wiki/India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14fd84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataset to txt file\n",
    "with open('BERT_AssignmentDataset.txt', 'w') as f:\n",
    "    f.write(datasetText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e470011",
   "metadata": {},
   "source": [
    "## Initialization & Dataset Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcad24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n",
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n",
      "creating dataset...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Methods / Class\n",
    "# =============================================================================\n",
    "def get_batch(loader, loader_iter):\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    return batch, loader_iter\n",
    "\n",
    "# =============================================================================\n",
    "# #Init\n",
    "# =============================================================================\n",
    "print('initializing..')\n",
    "batch_size = 1024\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n",
    "\n",
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'BERT_AssignmentDataset.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "#sentences = datasetText.lower().split(\"\\n\")\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab_bert_assigment11.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f913e",
   "metadata": {},
   "source": [
    "## Initialize Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7061c77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "## CPU Device\n",
    "DATA_DIR='./data'\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c00123df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "## Apple Silicon Metal Performance Shader\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "    \n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e4d85",
   "metadata": {},
   "source": [
    "## Model Initialization & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5414e7e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n",
      "initializing optimizer and loss...\n",
      "training...\n",
      "it: 0  | loss 8.04  | Δw: 3.759\n",
      "it: 10  | loss 7.19  | Δw: 3.045\n",
      "it: 20  | loss 6.64  | Δw: 3.123\n",
      "it: 30  | loss 6.19  | Δw: 3.218\n",
      "it: 40  | loss 5.78  | Δw: 3.091\n",
      "it: 50  | loss 5.43  | Δw: 2.951\n",
      "it: 60  | loss 5.12  | Δw: 2.848\n",
      "it: 70  | loss 4.85  | Δw: 2.753\n",
      "it: 80  | loss 4.6  | Δw: 2.644\n",
      "it: 90  | loss 4.42  | Δw: 2.611\n",
      "it: 100  | loss 4.21  | Δw: 2.52\n",
      "it: 110  | loss 4.03  | Δw: 2.474\n",
      "it: 120  | loss 3.86  | Δw: 2.452\n",
      "it: 130  | loss 3.67  | Δw: 2.364\n",
      "it: 140  | loss 3.55  | Δw: 2.343\n",
      "it: 150  | loss 3.39  | Δw: 2.284\n",
      "it: 160  | loss 3.23  | Δw: 2.236\n",
      "it: 170  | loss 3.14  | Δw: 2.199\n",
      "it: 180  | loss 2.98  | Δw: 2.152\n",
      "it: 190  | loss 2.9  | Δw: 2.126\n",
      "it: 200  | loss 2.81  | Δw: 2.087\n",
      "it: 210  | loss 2.7  | Δw: 2.037\n",
      "it: 220  | loss 2.58  | Δw: 1.989\n",
      "it: 230  | loss 2.49  | Δw: 1.951\n",
      "it: 240  | loss 2.38  | Δw: 1.922\n",
      "it: 250  | loss 2.31  | Δw: 1.878\n",
      "it: 260  | loss 2.2  | Δw: 1.823\n",
      "it: 270  | loss 2.15  | Δw: 1.795\n",
      "it: 280  | loss 2.08  | Δw: 1.759\n",
      "it: 290  | loss 2.0  | Δw: 1.739\n",
      "it: 300  | loss 1.94  | Δw: 1.741\n",
      "it: 310  | loss 1.88  | Δw: 1.68\n",
      "it: 320  | loss 1.84  | Δw: 1.674\n",
      "it: 330  | loss 1.77  | Δw: 1.62\n",
      "it: 340  | loss 1.73  | Δw: 1.586\n",
      "it: 350  | loss 1.67  | Δw: 1.554\n",
      "it: 360  | loss 1.63  | Δw: 1.525\n",
      "it: 370  | loss 1.54  | Δw: 1.513\n",
      "it: 380  | loss 1.54  | Δw: 1.506\n",
      "it: 390  | loss 1.5  | Δw: 1.489\n",
      "it: 400  | loss 1.48  | Δw: 1.462\n",
      "it: 410  | loss 1.44  | Δw: 1.466\n",
      "it: 420  | loss 1.4  | Δw: 1.435\n",
      "it: 430  | loss 1.39  | Δw: 1.442\n",
      "it: 440  | loss 1.33  | Δw: 1.366\n",
      "it: 450  | loss 1.35  | Δw: 1.421\n",
      "it: 460  | loss 1.31  | Δw: 1.366\n",
      "it: 470  | loss 1.25  | Δw: 1.372\n",
      "it: 480  | loss 1.25  | Δw: 1.331\n",
      "it: 490  | loss 1.24  | Δw: 1.326\n",
      "it: 500  | loss 1.19  | Δw: 1.317\n",
      "it: 510  | loss 1.2  | Δw: 1.329\n",
      "it: 520  | loss 1.17  | Δw: 1.326\n",
      "it: 530  | loss 1.12  | Δw: 1.26\n",
      "it: 540  | loss 1.08  | Δw: 1.222\n",
      "it: 550  | loss 1.11  | Δw: 1.266\n",
      "it: 560  | loss 1.09  | Δw: 1.229\n",
      "it: 570  | loss 1.07  | Δw: 1.199\n",
      "it: 580  | loss 1.07  | Δw: 1.199\n",
      "it: 590  | loss 1.02  | Δw: 1.21\n",
      "it: 600  | loss 1.05  | Δw: 1.211\n",
      "it: 610  | loss 1.01  | Δw: 1.119\n",
      "it: 620  | loss 0.99  | Δw: 1.134\n",
      "it: 630  | loss 0.98  | Δw: 1.114\n",
      "it: 640  | loss 0.96  | Δw: 1.117\n",
      "it: 650  | loss 0.98  | Δw: 1.136\n",
      "it: 660  | loss 0.98  | Δw: 1.094\n",
      "it: 670  | loss 0.94  | Δw: 1.085\n",
      "it: 680  | loss 0.93  | Δw: 1.073\n",
      "it: 690  | loss 0.93  | Δw: 1.064\n",
      "it: 700  | loss 0.94  | Δw: 1.08\n",
      "it: 710  | loss 0.9  | Δw: 1.046\n",
      "it: 720  | loss 0.94  | Δw: 1.079\n",
      "it: 730  | loss 0.9  | Δw: 1.073\n",
      "it: 740  | loss 0.87  | Δw: 1.033\n",
      "it: 750  | loss 0.9  | Δw: 1.065\n",
      "it: 760  | loss 0.89  | Δw: 1.038\n",
      "it: 770  | loss 0.86  | Δw: 1.067\n",
      "it: 780  | loss 0.86  | Δw: 1.034\n",
      "it: 790  | loss 0.85  | Δw: 1.056\n",
      "it: 800  | loss 0.87  | Δw: 1.064\n",
      "it: 810  | loss 0.83  | Δw: 1.02\n",
      "it: 820  | loss 0.82  | Δw: 1.033\n",
      "it: 830  | loss 0.82  | Δw: 1.039\n",
      "it: 840  | loss 0.78  | Δw: 1.016\n",
      "it: 850  | loss 0.81  | Δw: 1.036\n",
      "it: 860  | loss 0.81  | Δw: 0.999\n",
      "it: 870  | loss 0.77  | Δw: 1.011\n",
      "it: 880  | loss 0.8  | Δw: 1.033\n",
      "it: 890  | loss 0.77  | Δw: 1.019\n",
      "it: 900  | loss 0.77  | Δw: 1.012\n",
      "it: 910  | loss 0.78  | Δw: 1.042\n",
      "it: 920  | loss 0.77  | Δw: 1.026\n",
      "it: 930  | loss 0.78  | Δw: 1.042\n",
      "it: 940  | loss 0.75  | Δw: 1.021\n",
      "it: 950  | loss 0.74  | Δw: 1.002\n",
      "it: 960  | loss 0.73  | Δw: 1.001\n",
      "it: 970  | loss 0.72  | Δw: 1.012\n",
      "it: 980  | loss 0.72  | Δw: 1.014\n",
      "it: 990  | loss 0.71  | Δw: 1.003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "#init model\n",
    "print('initializing model...')\n",
    "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "#model = model.cuda()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
    "\n",
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 10\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 1000\n",
    "for it in range(n_iteration):\n",
    "    \n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "    \n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "    \n",
    "    #masked_input = masked_input.cuda(non_blocking=True)\n",
    "    #masked_target = masked_target.cuda(non_blocking=True)\n",
    "    masked_input.to(DEVICE)\n",
    "    masked_target.to(DEVICE)\n",
    "    \n",
    "    output = model(masked_input)\n",
    "    \n",
    "    #compute the cross entropy loss - original code\n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it, \n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cec0c",
   "metadata": {},
   "source": [
    "## Sample Inputs & Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694c82b",
   "metadata": {},
   "source": [
    "### Generate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c27c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 20, 2576])\n"
     ]
    }
   ],
   "source": [
    "batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "\n",
    "masked_input = batch['input']\n",
    "masked_target = batch['target']\n",
    "\n",
    "masked_input.to(DEVICE)\n",
    "\n",
    "output = model(masked_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39163112",
   "metadata": {},
   "source": [
    "### Convert Output to Token Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f72fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 20])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim = 2)\n",
    "probOutput = m(output)\n",
    "idxOutput = torch.argmax(probOutput, dim = 2)\n",
    "print(idxOutput.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1abc1",
   "metadata": {},
   "source": [
    "### Print Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c8c1ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Example 0---------------\n",
      "Input: government general information modern iconic arrived on the iconic iconic iconic iconic no later than 55 , 000 years ago\n",
      "Output: government general information modern humans arrived on the humans humans humans humans no later than 55 , 000 years ago\n",
      "------------Example 1---------------\n",
      "Input: many indian species have classical from classical of gondwana , the southern supercontinent from which india separated more than classical\n",
      "Output: many indian species have indian from , of gondwana , the southern supercontinent from which india separated more than indian\n",
      "------------Example 2---------------\n",
      "Input: elephanta caves , becoming becoming bust of shiva , 18 becoming tall , becoming .  550 becoming bronze of shiva\n",
      "Output: elephanta caves , and and bust of shiva , 18 , tall , and .  550 and bronze of shiva\n",
      "------------Example 3---------------\n",
      "Input: india mamuni substantially mamuni its rate of poverty , though at the cost of increasing economic mamuni . india is\n",
      "Output: india the substantially of its rate of poverty , though at the cost of increasing economic india . india is\n",
      "------------Example 4---------------\n",
      "Input: the education system of india is the world ' s second - largest . india has over 900 universities ,\n",
      "Output: the education system of india is the world ' s second - largest . india has over 900 universities ,\n",
      "------------Example 5---------------\n",
      "Input: the pharmaceutical industry in india reported reported a reported player . as of 2021 , with 3000 pharmaceutical reported and\n",
      "Output: the pharmaceutical industry in india reported reported a reported player . as of 2021 , with 3000 pharmaceutical reported and\n",
      "------------Example 6---------------\n",
      "Input: since the end of the cold p , india has increased its economic p strategic p and military co p\n",
      "Output: since the end of the cold , , india has increased its economic , strategic , and military co ,\n",
      "------------Example 7---------------\n",
      "Input: a platter , or thali , used 6° eating usually has 6° central place reserved for the 6° cereal 6°\n",
      "Output: a platter , or thali , used a eating usually has , central place reserved for the , cereal ,\n",
      "------------Example 8---------------\n",
      "Input: politics foreign relations and military economy demographics culture government general marma modern marma arrived on the indian subcontinent from africa\n",
      "Output: politics foreign relations and military economy demographics culture government general humans modern humans arrived on the indian subcontinent from africa\n",
      "------------Example 9---------------\n",
      "Input: india saracenic s land is megadiverse , with saracenic biodiversity hotspots . its forest cover comprises 21 . 7% of\n",
      "Output: india ' s land is megadiverse , with and biodiversity hotspots . its forest cover comprises 21 . 7% of\n"
     ]
    }
   ],
   "source": [
    " \n",
    "for i in range(10):\n",
    "    inp = masked_input[i]\n",
    "    outp = idxOutput[i]\n",
    "    \n",
    "    print(f\"------------Example {i}---------------\")\n",
    "    inp_text = [dataset.rvocab[int(idx)] for idx in inp]\n",
    "    print(\"Input: \" + ' '.join(inp_text))\n",
    "    outp_text = [dataset.rvocab[int(idx)] for idx in outp]\n",
    "    print(\"Output: \" + ' '.join(outp_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fe390",
   "metadata": {},
   "source": [
    "### Single Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "934c6080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 2576])\n",
      "torch.Size([1, 20])\n",
      "tensor([[ 869,  511,    0,    1,    3, 2113,    4,  366,    0,  881,    3, 2115,\n",
      "            0,    3,    2, 2116,    3,  871,    4,  366]])\n",
      "Input: elephanta caves , becoming becoming bust of shiva , 18 becoming tall , becoming .  550 becoming bronze of shiva\n",
      "Output: elephanta caves , the and bust of shiva , 18 and tall , and .  550 and bronze of shiva\n"
     ]
    }
   ],
   "source": [
    "model_inp = masked_input[2]\n",
    "model_out = model(model_inp)\n",
    "print(model_out.shape)\n",
    "mid = m(model_out)\n",
    "idxOut_model = torch.argmax(mid, dim = 2)\n",
    "print(idxOut_model.shape)\n",
    "print(idxOut_model)\n",
    "\n",
    "inp_text = [dataset.rvocab[int(idx)] for idx in model_inp]\n",
    "print(\"Input: \" + ' '.join(inp_text))\n",
    "outp_text = [dataset.rvocab[int(idx2)] for idx2 in idxOut_model[0]]\n",
    "print(\"Output: \" + ' '.join(outp_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
